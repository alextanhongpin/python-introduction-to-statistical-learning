{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3fbcef-b724-418d-becb-7ed87240d619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T08:46:23.300710Z",
     "iopub.status.busy": "2024-04-10T08:46:23.300144Z",
     "iopub.status.idle": "2024-04-10T08:46:26.121699Z",
     "shell.execute_reply": "2024-04-10T08:46:26.120733Z",
     "shell.execute_reply.started": "2024-04-10T08:46:23.300677Z"
    }
   },
   "outputs": [],
   "source": [
    "from ISLP import load_data\n",
    "from ISLP.models import ModelSpec as MS\n",
    "from ISLP.models import poly, summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d339e63-b0d0-4b2e-8988-f49ce585176f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T08:46:26.139363Z",
     "iopub.status.busy": "2024-04-10T08:46:26.123588Z",
     "iopub.status.idle": "2024-04-10T08:46:26.159092Z",
     "shell.execute_reply": "2024-04-10T08:46:26.155528Z",
     "shell.execute_reply.started": "2024-04-10T08:46:26.139313Z"
    }
   },
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c543bd2e-8a8a-469a-bc29-f7f4926d7e90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T08:46:32.462790Z",
     "iopub.status.busy": "2024-04-10T08:46:32.462168Z",
     "iopub.status.idle": "2024-04-10T08:46:32.552335Z",
     "shell.execute_reply": "2024-04-10T08:46:32.548985Z",
     "shell.execute_reply.started": "2024-04-10T08:46:32.462757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def load_data(dataset):\n",
      "    if dataset == 'NCI60':\n",
      "        with as_file(files('ISLP').joinpath('data', 'NCI60data.npy')) as features:\n",
      "            X = np.load(features)\n",
      "        with as_file(files('ISLP').joinpath('data', 'NCI60labs.csv')) as labels:\n",
      "            Y = pd.read_csv(labels)\n",
      "        return {'data':X, 'labels':Y}\n",
      "    elif dataset == 'Khan':\n",
      "        with as_file(files('ISLP').joinpath('data', 'Khan_xtest.csv')) as xtest:\n",
      "            xtest = pd.read_csv(xtest)\n",
      "        xtest = xtest.rename(columns=dict([('V%d' % d, 'G%04d' % d) for d in range(1, len(xtest.columns)+0)]))\n",
      "        with as_file(files('ISLP').joinpath('data', 'Khan_ytest.csv')) as ytest:\n",
      "            ytest = pd.read_csv(ytest)\n",
      "        ytest = ytest.rename(columns={'x':'Y'})\n",
      "        ytest = ytest['Y']\n",
      "        \n",
      "        with as_file(files('ISLP').joinpath('data', 'Khan_xtrain.csv')) as xtrain:\n",
      "            xtrain = pd.read_csv(xtrain)\n",
      "            xtrain = xtrain.rename(columns=dict([('V%d' % d, 'G%04d' % d) for d in range(1, len(xtest.columns)+0)]))\n",
      "\n",
      "        with as_file(files('ISLP').joinpath('data', 'Khan_ytrain.csv')) as ytrain:\n",
      "            ytrain = pd.read_csv(ytrain)\n",
      "        ytrain = ytrain.rename(columns={'x':'Y'})\n",
      "        ytrain = ytrain['Y']\n",
      "\n",
      "        return {'xtest':xtest,\n",
      "                'xtrain':xtrain,\n",
      "                'ytest':ytest,\n",
      "                'ytrain':ytrain}\n",
      "\n",
      "    elif dataset == 'Hitters':\n",
      "        with as_file(files('ISLP').joinpath('data', '%s.csv' % dataset)) as filename:\n",
      "            df = pd.read_csv(filename)\n",
      "        for col in ['League', 'Division', 'NewLeague']:\n",
      "            df[col] = pd.Categorical(df[col])\n",
      "        return df\n",
      "    elif dataset == 'Carseats':\n",
      "        with as_file(files('ISLP').joinpath('data', '%s.csv' % dataset)) as filename:\n",
      "            df = pd.read_csv(filename)\n",
      "        for col in ['ShelveLoc', 'Urban', 'US']:\n",
      "            df[col] = pd.Categorical(df[col])\n",
      "        return df\n",
      "    elif dataset == 'NYSE':\n",
      "        with as_file(files('ISLP').joinpath('data', '%s.csv' % dataset)) as filename: \n",
      "            df = pd.read_csv(filename).set_index('date')\n",
      "        return df\n",
      "    elif dataset == 'Publication':\n",
      "        with as_file(files('ISLP').joinpath('data', 'Publication.csv')) as f:\n",
      "            df = pd.read_csv(f)\n",
      "        for col in ['mech']:\n",
      "            df[col] = pd.Categorical(df[col])\n",
      "        return df\n",
      "    elif dataset == 'BrainCancer':\n",
      "        with as_file(files('ISLP').joinpath('data', 'BrainCancer.csv')) as f:\n",
      "            df = pd.read_csv(f)\n",
      "        for col in ['sex', 'diagnosis', 'loc', 'stereo']:\n",
      "            df[col] = pd.Categorical(df[col])\n",
      "        return df\n",
      "    elif dataset == 'Bikeshare':\n",
      "        with as_file(files('ISLP').joinpath('data', '%s.csv' % dataset)) as filename:\n",
      "            df = pd.read_csv(filename)\n",
      "        df['weathersit'] = pd.Categorical(df['weathersit'], ordered=False)\n",
      "        # setting order to avoid alphabetical\n",
      "        df['mnth'] = pd.Categorical(df['mnth'],\n",
      "                                    ordered=False,\n",
      "                                    categories=['Jan', 'Feb',\n",
      "                                                'March', 'April',\n",
      "                                                'May', 'June',\n",
      "                                                'July', 'Aug',\n",
      "                                                'Sept', 'Oct',\n",
      "                                                'Nov', 'Dec'])\n",
      "        df['hr'] = pd.Categorical(df['hr'],\n",
      "                                  ordered=False,\n",
      "                                  categories=range(24))\n",
      "        return df\n",
      "    elif dataset == 'Wage':\n",
      "        with as_file(files('ISLP').joinpath('data', 'Wage.csv')) as f:\n",
      "            df = pd.read_csv(f)\n",
      "            df['education'] = pd.Categorical(df['education'], ordered=True)\n",
      "        return df\n",
      "    else:\n",
      "        with as_file(files('ISLP').joinpath('data', '%s.csv' % dataset)) as filename:\n",
      "            return pd.read_csv(filename)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(load_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfcf1a55-8a8c-47c8-9bfd-88f02c9fc44c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T08:46:40.063756Z",
     "iopub.status.busy": "2024-04-10T08:46:40.038042Z",
     "iopub.status.idle": "2024-04-10T08:46:40.129624Z",
     "shell.execute_reply": "2024-04-10T08:46:40.127853Z",
     "shell.execute_reply.started": "2024-04-10T08:46:40.063706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ModelSpec(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    '''\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    terms : sequence (optional)\n",
      "        Sequence of sets whose\n",
      "        elements are columns of *X* when fit.\n",
      "        For :py:class:`pd.DataFrame` these can be column\n",
      "        names.\n",
      "\n",
      "    intercept : bool (optional)\n",
      "        Include a column for intercept?\n",
      "\n",
      "    categorical_features : array-like of {bool, int} of shape (n_features) \n",
      "            or shape (n_categorical_features,), default=None.\n",
      "        Indicates the categorical features. Will be ignored if *X* is a :py:class:`pd.DataFrame`\n",
      "        or :py:class:`pd.Series`.\n",
      "\n",
      "        - None : no feature will be considered categorical for :py:class:`np.ndarray`.\n",
      "        - boolean array-like : boolean mask indicating categorical features.\n",
      "        - integer array-like : integer indices indicating categorical\n",
      "          features.\n",
      "\n",
      "    default_encoders : dict\n",
      "        Dictionary whose keys are elements of *terms* and values\n",
      "        are transforms to be applied to the associate columns in the model matrix\n",
      "        by running the *fit_transform* method when *fit* is called and overwriting\n",
      "        these values in the dictionary.\n",
      "    '''\n",
      "\n",
      "    def __init__(self,\n",
      "                 terms=[],\n",
      "                 intercept=True,\n",
      "                 categorical_features=None,\n",
      "                 default_encoders={'categorical': Contrast(method='drop'),\n",
      "                                   'ordinal': OrdinalEncoder()}\n",
      "                 ):\n",
      "       \n",
      "        self.intercept = intercept\n",
      "        self.terms = terms\n",
      "        self.categorical_features = categorical_features\n",
      "        self.default_encoders = default_encoders\n",
      "        \n",
      "    def fit(self, X, y=None):\n",
      "\n",
      "        \"\"\"\n",
      "        Construct parameters for orthogonal\n",
      "        polynomials in the feature X.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like\n",
      "            X on which model matrix will be evaluated.\n",
      "            If a :py:class:`pd.DataFrame` or :py:class:`pd.Series`, variables that are of\n",
      "            categorical dtype will be treated as categorical.\n",
      "\n",
      "        \"\"\"\n",
      "        \n",
      "        if isinstance(X, (pd.DataFrame, pd.Series)):\n",
      "            (categorical_features,\n",
      "             self.is_ordinal_) = _categorical_from_df(X)\n",
      "            (self.is_categorical_,\n",
      "             self.known_categories_) = _check_categories(categorical_features,\n",
      "                                                         X)\n",
      "            self.columns_ = X.columns\n",
      "            if self.is_categorical_ is None:\n",
      "                self.is_categorical_ = np.zeros(X.shape[1], bool)\n",
      "            self.is_ordinal_ = pd.Series(self.is_ordinal_,\n",
      "                                         index=self.columns_)\n",
      "            self.is_categorical_ = pd.Series(self.is_categorical_,\n",
      "                                             index=self.columns_)\n",
      "        else:\n",
      "            categorical_features = self.categorical_features\n",
      "            (self.is_categorical_,\n",
      "             self.known_categories_) = _check_categories(categorical_features,\n",
      "                                                         X)\n",
      "            if self.is_categorical_ is None:\n",
      "                self.is_categorical_ = np.zeros(X.shape[1], bool)\n",
      "            self.is_ordinal_ = np.zeros(self.is_categorical_.shape,\n",
      "                                        bool)\n",
      "            self.columns_ = np.arange(X.shape[1])\n",
      "\n",
      "        self.features_ = {}\n",
      "        self.encoders_ = {}\n",
      "\n",
      "        self.column_info_ = _get_column_info(X,\n",
      "                                             self.columns_,\n",
      "                                             self.is_categorical_,\n",
      "                                             self.is_ordinal_,\n",
      "                                             default_encoders=self.default_encoders)\n",
      "        # include each column as a Feature\n",
      "        # so that their columns are built if needed\n",
      "\n",
      "        for col_ in self.columns_:\n",
      "            self.features_[col_] = Feature((col_,), str(col_), None, pure_columns=True) \n",
      "\n",
      "        # find possible interactions and other features\n",
      "\n",
      "        tmp_cache = {}\n",
      "\n",
      "        for term in self.terms:\n",
      "            if isinstance(term, Feature):\n",
      "                self.features_[term] = term\n",
      "                build_columns(self.column_info_,\n",
      "                              X,\n",
      "                              term,\n",
      "                              encoders=self.encoders_,\n",
      "                              col_cache=tmp_cache,\n",
      "                              fit=True) # these encoders won't have been fit yet\n",
      "                for var in term.variables:\n",
      "                    if var not in self.features_ and isinstance(var, Feature):\n",
      "                            self.features_[var] = var\n",
      "            elif term not in self.column_info_:\n",
      "                # a tuple of features represents an interaction\n",
      "                if type(term) == type((1,)): \n",
      "                    names = []\n",
      "                    column_map = {}\n",
      "                    column_names = {}\n",
      "                    idx = 0\n",
      "                    for var in term:\n",
      "                        if var in self.features_:\n",
      "                            var = self.features_[var]\n",
      "                        cols, cur_names = build_columns(self.column_info_,\n",
      "                                                        X,\n",
      "                                                        var,\n",
      "                                                        encoders=self.encoders_,\n",
      "                                                        col_cache=tmp_cache,\n",
      "                                                        fit=True) # these encoders won't have been fit yet\n",
      "                        column_map[var.name] = range(idx, idx + cols.shape[1])\n",
      "                        column_names[var.name] = cur_names\n",
      "                        idx += cols.shape[1]                 \n",
      "                        names.append(var.name)\n",
      "                    encoder_ = Interaction(names, column_map, column_names)\n",
      "                    self.features_[term] = Feature(term, ':'.join(n for n in names), encoder_)\n",
      "                elif isinstance(term, Column):\n",
      "                    self.features_[term] = Feature((term,), term.name, None, pure_columns=True)\n",
      "                else:\n",
      "                    raise ValueError('each element in a term should be a Feature, Column or identify a column')\n",
      "                \n",
      "        # build the mapping of terms to columns and column names\n",
      "\n",
      "        self.column_names_ = {}\n",
      "        self.column_map_ = {}\n",
      "        self.terms_ = [self.features_[t] for t in self.terms]\n",
      "        \n",
      "        idx = 0\n",
      "        if self.intercept:\n",
      "            self.column_map_['intercept'] = slice(0, 1)\n",
      "            idx += 1 # intercept will be first column\n",
      "        \n",
      "        for term, term_ in zip(self.terms, self.terms_):\n",
      "            term_df, term_names = build_columns(self.column_info_,\n",
      "                                                X,\n",
      "                                                term_,\n",
      "                                                encoders=self.encoders_)\n",
      "            self.column_names_[term] = term_names\n",
      "            self.column_map_[term] = slice(idx, idx + term_df.shape[1])\n",
      "            idx += term_df.shape[1]\n",
      "    \n",
      "        return self\n",
      "    \n",
      "    def transform(self, X, y=None):\n",
      "        \"\"\"\n",
      "        Build design on X after fitting.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like\n",
      "\n",
      "        y : None\n",
      "            Ignored. This parameter exists only for compatibility with\n",
      "            :py:class:`sklearn.pipeline.Pipeline`.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "        return build_model(self.column_info_,\n",
      "                           X,\n",
      "                           self.terms_,\n",
      "                           intercept=self.intercept,\n",
      "                           encoders=self.encoders_)\n",
      "    \n",
      "    # ModelSpec specific methods\n",
      "\n",
      "    @property\n",
      "    def names(self, help='Name for each term in model specification.'):\n",
      "        names = []\n",
      "        if self.intercept:\n",
      "            names = ['intercept']\n",
      "        return names + [t.name for t in self.terms_]\n",
      "        \n",
      "\n",
      "    def build_submodel(self,\n",
      "                       X,\n",
      "                       terms):\n",
      "        \"\"\"\n",
      "        Build design on X after fitting.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like\n",
      "            X on which columns are evaluated.\n",
      "\n",
      "        terms : [Feature]\n",
      "            Sequence of features\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        D : array-like\n",
      "            Design matrix created with `terms`\n",
      "        \"\"\"\n",
      "\n",
      "        return build_model(self.column_info_,\n",
      "                           X,\n",
      "                           terms,\n",
      "                           intercept=self.intercept,\n",
      "                           encoders=self.encoders_)\n",
      "\n",
      "    def build_sequence(self,\n",
      "                       X,\n",
      "                       anova_type='sequential'):\n",
      "        \"\"\"\n",
      "        Build implied sequence of submodels \n",
      "        based on successively including more terms.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like\n",
      "            X on which columns are evaluated.\n",
      "\n",
      "        anova_type: str\n",
      "            One of \"sequential\" or \"drop\".\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "\n",
      "        models : generator\n",
      "            Generator for sequence of models for ANOVA.\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        col_cache = {}  # avoid recomputing the same columns\n",
      "\n",
      "        dfs = []\n",
      "\n",
      "        if self.intercept:\n",
      "            df_int = pd.DataFrame({'intercept':np.ones(X.shape[0])})\n",
      "            if isinstance(X, (pd.Series, pd.DataFrame)):\n",
      "                df_int.index = X.index\n",
      "            dfs.append(df_int)\n",
      "        else:\n",
      "            df_int = pd.DataFrame({'zero':np.zeros(X.shape[0])})\n",
      "            if isinstance(X, (pd.Series, pd.DataFrame)):\n",
      "                df_int.index = X.index\n",
      "            dfs.append(df_int)\n",
      "\n",
      "        for term_ in self.terms_:\n",
      "            term_df, _  = build_columns(self.column_info_,\n",
      "                                        X,\n",
      "                                        term_,\n",
      "                                        col_cache=col_cache,\n",
      "                                        encoders=self.encoders_,\n",
      "                                        fit=False)\n",
      "            if isinstance(X, (pd.Series, pd.DataFrame)):\n",
      "                term_df.index = X.index\n",
      "\n",
      "            dfs.append(term_df)\n",
      "\n",
      "        if anova_type == 'sequential':\n",
      "            if isinstance(X, (pd.Series, pd.DataFrame)):\n",
      "                return (pd.concat(dfs[:i], axis=1) for i in range(1, len(dfs)+1))\n",
      "            else:\n",
      "                return (np.column_stack(dfs[:i]) for i in range(1, len(dfs)+1))\n",
      "        elif anova_type == 'drop':\n",
      "            if isinstance(X, (pd.Series, pd.DataFrame)):\n",
      "                return (pd.concat([dfs[j] for j in range(len(dfs)) if j != i], axis=1) for i in range(len(dfs)))\n",
      "            else:\n",
      "                return (np.column_stack([dfs[j] for j in range(len(dfs)) if j != i]) for i in range(len(dfs)))\n",
      "        else:\n",
      "            raise ValueError('anova_type must be one of [\"sequential\", \"drop\"]')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(MS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b849c5-e99c-454d-99b2-2ffbf515830b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T08:46:43.951219Z",
     "iopub.status.busy": "2024-04-10T08:46:43.950356Z",
     "iopub.status.idle": "2024-04-10T08:46:43.978477Z",
     "shell.execute_reply": "2024-04-10T08:46:43.976913Z",
     "shell.execute_reply.started": "2024-04-10T08:46:43.951164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def poly(col,\n",
      "         degree=1,\n",
      "         intercept=False,\n",
      "         raw=False,\n",
      "         name=None):\n",
      "\n",
      "    \"\"\"\n",
      "    Create a polynomial Feature\n",
      "    for a given column.\n",
      "    \n",
      "    Additional `args` and `kwargs`\n",
      "    are passed to `Poly`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    col : column identifier or Column\n",
      "        Column to transform.\n",
      "\n",
      "    degree : int, default=1\n",
      "        Degree of polynomial.\n",
      "\n",
      "    intercept : bool, default=False\n",
      "        Include a column for intercept?\n",
      "\n",
      "    raw : bool, default=False\n",
      "        If False, perform a QR decomposition on the resulting\n",
      "        matrix of powers of centered and / or scaled features.\n",
      "\n",
      "    name : str (optional)\n",
      "        Defaults to one derived from col.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "\n",
      "    var : Feature\n",
      "    \"\"\"\n",
      "    shortname, klass = 'poly', Poly\n",
      "    encoder = klass(degree=degree,\n",
      "                    raw=raw,\n",
      "                    intercept=intercept) \n",
      "    if name is None:\n",
      "        if isinstance(col, Column):\n",
      "            name = col.name\n",
      "        else:\n",
      "            name = str(col)\n",
      "\n",
      "        kwargs = {}\n",
      "        if intercept:\n",
      "            kwargs['intercept'] = True\n",
      "        if raw:\n",
      "            kwargs['raw'] = True\n",
      "\n",
      "        _args = _argstring(degree=degree,\n",
      "                           **kwargs)\n",
      "        if _args:\n",
      "            name = ', '.join([name, _args])\n",
      "\n",
      "        name = f'{shortname}({name})'\n",
      "\n",
      "    return derived_feature([col],\n",
      "                            name=name,\n",
      "                            encoder=encoder)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e16a4e3-37a0-47da-bf5a-75a872778e16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-10T08:46:46.949855Z",
     "iopub.status.busy": "2024-04-10T08:46:46.943876Z",
     "iopub.status.idle": "2024-04-10T08:46:46.990415Z",
     "shell.execute_reply": "2024-04-10T08:46:46.980085Z",
     "shell.execute_reply.started": "2024-04-10T08:46:46.949737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def summarize(results,\n",
      "              conf_int=False):\n",
      "    \"\"\"\n",
      "    Take a fit statsmodels and summarize it\n",
      "    by returning the usual coefficient estimates,\n",
      "    their standard errors, the usual test\n",
      "    statistics and P-values as well as \n",
      "    (optionally) 95% confidence intervals.\n",
      "\n",
      "    Based on:\n",
      "\n",
      "    https://stackoverflow.com/questions/51734180/converting-statsmodels-summary-object-to-pandas-dataframe\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "\n",
      "    results : a results object\n",
      "\n",
      "    conf_int : bool (optional)\n",
      "        Include 95% confidence intervals?\n",
      "\n",
      "    \"\"\"\n",
      "    tab = results.summary().tables[1]\n",
      "    results_table = pd.read_html(tab.as_html(),\n",
      "                                 index_col=0,\n",
      "                                 header=0)[0]\n",
      "    if not conf_int:\n",
      "        columns = ['coef',\n",
      "                   'std err',\n",
      "                   't',\n",
      "                   'P>|t|']\n",
      "        return results_table[results_table.columns[:-2]]\n",
      "    return results_table\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(summarize))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
